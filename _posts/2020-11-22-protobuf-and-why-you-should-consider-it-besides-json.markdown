---
layout: post
title:  "Protobuf, and why you should consider it besides JSON"
date:   2020-11-22
author: "Niels Delestinne"
categories: java api protobuf
comments: true
---

Protocol Buffers offers a schema-driven method to serialize data using an optimized 
binary encoding. It offers speed, size efficiency, type-safety and simplicity. Think XML but smaller & faster. 
Think JSON, but with different tradeoffs.

## API Compatibility

In a previous article, I wrote about [asserting API compatibility when using JSON as the data-interchange format]({% post_url 2020-11-21-asserting-api-compatibility-when-using-json %}). 

> When working with HTTP-based REST(ful) web APIs or even with an event-driven architecture, JSON is frequently used as the 
data-interchange format. The exchanged JSON messages themselves are often the implicit 'contracts' between the producer and consumer.
Not having an explicit contract makes it hard to enforce and maintain **API compatibility** between producer and consumer(s), the 'contract' can be easily broken by accident. 

In that article, I discussed the problems (and some solutions) to not having an explicit contract for the (JSON) messages your applications exchange (though their APIs).
One of these solutions was using JSON schema to explicitly define the data structure of every message. 
However, JSON schema is verbose and offers no additional benefits, besides more easily asserting API compatibility.

Let's find out how Protocol Buffers (a.k.a. **Protobuf**) offers the same - **and more** - benefits!  

## What is Protobuf?

Protobuf is a lightweight data-interchange format that can be used for both transport and storage, a description that would also fit JSON.
But, **Protobuf is more than that...**

### Schema

Protobuf is contract-driven by design. It provides a simple & readable Intermediate Definition Language (IDL) that 
is used to create **proto definition files** (schemas) that define that structure of the data (message).

### Binary encoding

Protobuf has an **optimized binary encoding**, making...
- Every serialized message **smaller** in size (compared to JSON)
- The serialization & deserialization process faster (compared to Jackson)
- A serialized message unreadable for humans (there are some partial solutions)

### Some History

Protobuf is created in 2001 by Google for internal use and open sourced in 2008 as version 2 (Proto2). 
It is a battle-proven (read: not a novelty) tool that is used by (among others) Netflix and Cisco. 
- In 2016, version 3 (Proto3) was released
- Version 4 was planned for August 2020 but ultimately released as 3.13
- The latest release (at the time of writing this article) is 3.14 (14 November 2020)

Protobuf is designed and built together with gRPC, be it cleanly separated and modular.
gRPC is an open-source, high-performance RPC framework. *Think HTTP-based REST(ful) Web APIs but action-based, not resource-based.*

## How to use Protobuf?

Using Protobuf involves a few steps of which some can be left out, let's discover an approach that involves 5:
1. Define the contract (schema)
2. Generated (Java) sources based on the defined contract
3. Package, release & distribute the sources (and schema) as an artifact
4. Start using the artifact in your producer & consumer codebase
5. Start exchanging Protobuf encoded messages

![How to use Protobuf](/assets/img/2020-11-22/how-protobuf.jpg)

### Define the contract

Using Protobuf, we have to define our message's data structure up front in so-called **proto definition files** (`.proto`). 
Protobuf has its own lightweight and simple IDL to define these data structures, as demonstrated below:

{% highlight proto %}
// shape.proto​

syntax = "proto3";​
​
message Point {​
  sint32 x = 1;​
  sint32 y = 2;​
}​
​
message Line {​
  Point start = 1;​
  Point end = 2;​
  string label = 3;​
}​
​
message Shape {​
  repeated Line lines = 1;​
  bool isEnclosed = 2;​
  string label = 3;​
}
{% endhighlight %}

Displayed above is the `shape.proto` **schema** that serves as the official contract of the `Point`, `Line` and `Shape` messages.
- A Protobuf definition file contains one or more `message` definitions
- A `message` contains on or more **fields**
- A **field** consists of a **type**, a **name** and a **field tag**
- A **type** is numerical (double, float, int32, int64, sint32,...), bool, string, bytes, enumeration, another (common) message, map, etc...
- A **field** is **singular** or **repeated**
- A **field tag** identifies the field in the binary encoded format and should never be changed or reused (the **name** is only used for human visual identification)

Using this schema, (Java) sources can be generated by the Protobuf compiler.

### Compile into sources

The **Protobuf Compiler** (`protoc`) uses the schema (`.proto`) as input to generate Java sources (or sources of other supported languages).
Using the `shape.proto` definition file as input, the Protobuf compiler will generate a `Point.java`, `Line.java` and `Shape.java` class file. 
Each such class represents a message & contains:
- A builder​ to construct a Java instance of the class
- A method to serialize the object into bytes that are encoded using Protobuf's optimized binary encoding method
- A method to deserialize Protobuf encoded bytes into a new Java instance of the class

### Package, release and distribute

Although one could argue that we could solely share the `.proto` definition files and to let the 'using' applications generate the sources themselves, I do believe there to be benefit in 
packaging & releasing both the definition files and the generated sources.
- Version, package and release a Gradle or Maven (Java) module containing both the definitions files as the generated sources of a specific producer
- An advantage of packaging the generated sources as well, is that using applications do not have to use (or worry about) the Protobuf compiler. 
- A disadvantage of this approach is that a future change to the contract might not be breaking as far as Protobuf is concerned, but it might actually break compilation due to the re-generated sources (e.g. a renamed field). 

### Use the generated sources

Start using the released artifact in both your producer & consumer codebase.
- Use the builders to create instances
- Use the built-in serialization (`toByteArray()`) and deserialization methods right before sending or accepting messages

{% highlight java %}
var shape = Shape.newBuilder()​
        .setIsEnclosed(false)​
        .addLines(Line.newBuilder()​
                .setLabel("width")​
                .setStart(Point.newBuilder()​
                        .setX(0)​
                        .setY(0)​
                        .build())​
                .setEnd(Point.newBuilder()​
                        .setX(25)​
                        .setY(0)​
                        .build())​
                .build())​
        .addLines(Line.newBuilder()​
                .setLabel("Height")​
                .setStart(Point.newBuilder()​
                        .setX(0)​
                        .setY(0)​
                        .build())​
                .setEnd(Point.newBuilder()​
                        .setX(0)​
                        .setY(50)​
                        .build())​
                .build())​
        .build();​
​
return shape.toByteArray();
{% endhighlight %}

By guaranteeing the same version of the artifact is used by both producer as consumer(s), 
you guarantee API compatibility. *And even when different versions are used, we should be relatively safe as Protobuf offers backward and forward compatibility.*

## Why use Protobuf?

With Protobuf, API compatibility comes *for free*, which is a great advantage. 
Protobuf does offer some interesting additional advantages which justify its use. 
We will cover the performance benefits regarding (de)serialization, and the reduced message size. 
Furthermore we will look into Protobuf's schema evolution support. 

### Size

Let's perform a simplistic size comparison with JSON. We are going to create two messages with the exact same content:
1. One using JSON (which will encode as UTF-8 to get the amount of bytes)
2. One using Protobuf (encoded using Protobuf's binary encoding method)

The JSON message looks as follows:
{% highlight proto %}
{
  "id": {
    "value": "123"
  },
  "employerName": "Building Corporation Y",
  "size": "MEDIUM",
  "isFamilyOwned": true
}
{% endhighlight %}

The Protobuf schema and builder-constructed object (which will be encoded to the unreadable binary format):
{% highlight proto %}
syntax = "proto3";

message Id {
  string value = 1;
}

message CreateEmployerRequest {
  Id id = 1;
  string employerName = 2;
  EmployerSize size = 3;
  bool isFamilyOwned = 4;
}

enum EmployerSize {
  SMALL = 0;
  MEDIUM = 1;
  LARGE = 2;
  XLARGE = 3;
}
{% endhighlight %}
{% highlight java %}
CreateEmployerRequest.newBuilder()
    .setId(Id.newBuilder().setValue("123"))
    .setEmployerName("Building Corporation Y")
    .setIsFamilyOwned(true)
    .setSize(MEDIUM)
    .build();
{% endhighlight %}

So, we have generated two messages that contain the exact same content, one using JSON (text) and one using Protobuf (binary).
The difference in size between both is quite large:
- JSON message size: **125** bytes (included whitespaces, UTF-8 encoded)
- Protobuf message size: **35** bytes

In this particular example, the Protobuf message is 3,57 times smaller than the JSON message.

Protobuf realizes this size difference by using variable length encoding (e.g. [varints](https://developers.google.com/protocol-buffers/docs/encoding#varints))
and using field tags instead of field names (among other techniques). Based on the data (content) used when comparing JSON's with Protobuf's message size, the results can vary. 
Therefore, we have to be careful with drawing conclusions based on a single scenario (it's more of an observation than a conclusion).
I did perform some [additional size comparisons](https://github.com/nielsdelestinne/proto) in which I used data with zero to high duplication, compressed (GZIP) versus uncompressed.
- No matter the setup, Protobuf produces smaller messages than JSON. However, when the data contains a lot of duplication and the message is compressed, the difference becomes minimal. 

We can however correctly state that Protobuf does produce smaller messages than JSON.

### Performance

How does Protobuf compare to Jackson when it comes to serialization and deserialization performance?​
​- For Protobuf, the Java sources generated by the Protobuf compiler contain methods for (de)serialization.
- For JSON, Jackson's `ObjectMapper` allows to (de)serialize JSON​

​I've [performed a performance benchmark](https://github.com/nielsdelestinne/proto) using Java Microbenchmark Harness (JMH)​:
- Protobuf's serialization was **3 times faster** than Jackson's serialization
- Protobuf's deserialization was **2 times faster** than Jackson's deserialization

### Schema evolution

With Protobuf's schema evolution, producers & consumers can have different schema versions​ at the same time​ and it all keeps working.
Protobuf is designed to be both backward and forward compatible​ and can deal with newly added, deleted and even renamed fields without crashing.

> Backward compatibility: Messages designed for an older version of the system, are still accepted & processed by a newer version of that same system.

Protobuf's **backward compatibility** (from the perspective of the Employment Service):
- Missing fields (newly added) receive their default type value​ (`size` and `isFamilyOwned`)
- Renamed fields are processed without issue​ (`name` to `employerName`)
- Unexpected fields (deleted) are ignored (`isBel20`)

![Protobuf Backward Compatibility](/assets/img/2020-11-22/protobuf-backward-compatibility.jpg)

> Forward compatibility: A system is already able to accept & process a message, designed for a future version of that system.

Protobuf's **forward compatibility** (from the perspective of the Employment Service):
- Missing fields (deleted) receive their default type value​ (`isBel20`)
- Renamed fields are processed without issue​ (`name` to `employerName`)
- Unexpected fields (newly added) are ignored (`size` and `isFamilyOwned`)

![Protobuf Forward Compatibility](/assets/img/2020-11-22/protobuf-forward-compatibility.jpg)

To ensure backward and forward compatibility it is important to [follow the proper guidelines and rules of Protobuf](https://developers.google.com/protocol-buffers/docs/proto3#updating).
E.g. reusing or changing the field tags after they have been used is not allowed.

*JSON & Jackson also offer backward & forward compatibility, but it's more limited and more up to the developers to handle.* 

## When not to use Protobuf?

Every job requires the right tool, and although I have emphasised the benefits of Protobuf in this article, 
Protobuf is no silver bullet.

**Protobuf can be a good fit** when...
- Your project consists of a multi-service application landscape that contains a lot of internal APIs.
- API compatibility is a recurring issue for which you haven't yet found a satisfactory solution.
- You want to more easily support schema evolution
- You require better performance and a reduced message size (many other solutions exist as well) 

**Protobuf is less of a fit** when... 
- We are creating public APIs. Protobuf encoded messages are not human readable, but binary. 
*Proto3 does support JSON encoding, allowing to easily encode/decode to and from JSON. Which is ideal if we want to fulfil some kind of human readable test-support needs. Not really suited for production needs though.* 
- We are creating APIs for JavaScript-based clients (as JSON might be better suited here).
- We require streaming access. Unfortunately, Protobuf does not support random / streaming access. This means we have 
to decode an entire message into memory if even we only want to access a small portion its content.   

## Resources

The following resources contain more information on the discussed topics above:
- [Protocol Buffers](https://developers.google.com/protocol-buffers) 
- [Accompanying Code (GitHub)](https://github.com/nielsdelestinne/proto) 
